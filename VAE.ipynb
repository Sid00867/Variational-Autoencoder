{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**My Implementation of the Variational Auto Encoder**\n",
        "\n",
        "[Link To the Paper - Kingma & Welling (2013)](https://arxiv.org/pdf/1312.6114)"
      ],
      "metadata": {
        "id": "tUJhi51fexGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T27hGF6-s8kb",
        "outputId": "62223ad0-60f2-4c3b-9e64-b0c109e36a30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as mp\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "latent_dim = 64\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 60\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "5QdBbmsTeupm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(128*4*4, 256)\n",
        "        self.mu = nn.Linear(256, latent_dim)\n",
        "        self.logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # decoder\n",
        "\n",
        "        self.fcinv = nn.Linear(latent_dim, 256)\n",
        "        self.fcinv2 = nn.Linear(256, 128*4*4)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward_training(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        h = self.fc(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "\n",
        "        z = mu + torch.exp(logvar/2) * torch.randn_like(mu)\n",
        "\n",
        "        z = self.fcinv(z)\n",
        "        z = self.fcinv2(z)\n",
        "        z = z.view(z.size(0), 128, 4, 4)\n",
        "\n",
        "        x = self.decoder(z)\n",
        "\n",
        "        return x, mu, logvar"
      ],
      "metadata": {
        "id": "fClSTMH_f2BM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE().to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def loss_fn(x_hat, x, mu, logvar):\n",
        "    recon = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon + kl\n",
        "\n",
        "def train():\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train() #TRAINING MODE for model (enables dropout and batch norm) --- model.eval() for inference\n",
        "        total_loss = 0\n",
        "\n",
        "        for x, _ in train_loader:\n",
        "            x = x.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "\n",
        "            x_hat, mu, logvar = model.forward_training(x)\n",
        "            loss = loss_fn(x_hat, x, mu, logvar)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg = total_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/vae_cifar10_2.pth\")\n",
        ""
      ],
      "metadata": {
        "id": "sO56GV_xXnzx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTHCzG8msWu8",
        "outputId": "1f2b16a6-d32a-4c1a-cc57-28851c941881"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60 | Loss: 73.9501\n",
            "Epoch 2/60 | Loss: 73.9006\n",
            "Epoch 3/60 | Loss: 73.8839\n",
            "Epoch 4/60 | Loss: 73.8239\n",
            "Epoch 5/60 | Loss: 73.8484\n",
            "Epoch 6/60 | Loss: 73.8927\n",
            "Epoch 7/60 | Loss: 73.7871\n",
            "Epoch 8/60 | Loss: 73.8504\n",
            "Epoch 9/60 | Loss: 73.7696\n",
            "Epoch 10/60 | Loss: 73.7463\n",
            "Epoch 11/60 | Loss: 73.7254\n",
            "Epoch 12/60 | Loss: 73.7532\n",
            "Epoch 13/60 | Loss: 73.7823\n",
            "Epoch 14/60 | Loss: 73.7002\n",
            "Epoch 15/60 | Loss: 73.7247\n",
            "Epoch 16/60 | Loss: 73.6939\n",
            "Epoch 17/60 | Loss: 73.7070\n",
            "Epoch 18/60 | Loss: 73.7142\n",
            "Epoch 19/60 | Loss: 73.6935\n",
            "Epoch 20/60 | Loss: 73.6101\n",
            "Epoch 21/60 | Loss: 73.6808\n",
            "Epoch 22/60 | Loss: 73.6247\n",
            "Epoch 23/60 | Loss: 73.7283\n",
            "Epoch 24/60 | Loss: 73.6193\n",
            "Epoch 25/60 | Loss: 73.5811\n",
            "Epoch 26/60 | Loss: 73.5708\n",
            "Epoch 27/60 | Loss: 73.5548\n",
            "Epoch 28/60 | Loss: 73.5522\n",
            "Epoch 29/60 | Loss: 73.5548\n",
            "Epoch 30/60 | Loss: 73.5700\n",
            "Epoch 31/60 | Loss: 73.5603\n",
            "Epoch 32/60 | Loss: 73.5571\n",
            "Epoch 33/60 | Loss: 73.5403\n",
            "Epoch 34/60 | Loss: 73.4641\n",
            "Epoch 35/60 | Loss: 73.4843\n",
            "Epoch 36/60 | Loss: 73.5010\n",
            "Epoch 37/60 | Loss: 73.5211\n",
            "Epoch 38/60 | Loss: 73.5177\n",
            "Epoch 39/60 | Loss: 73.4771\n",
            "Epoch 40/60 | Loss: 73.4972\n",
            "Epoch 41/60 | Loss: 73.4468\n",
            "Epoch 42/60 | Loss: 73.4580\n",
            "Epoch 43/60 | Loss: 73.4607\n",
            "Epoch 44/60 | Loss: 73.4048\n",
            "Epoch 45/60 | Loss: 73.3926\n",
            "Epoch 46/60 | Loss: 73.4692\n",
            "Epoch 47/60 | Loss: 73.4210\n",
            "Epoch 48/60 | Loss: 73.4248\n",
            "Epoch 49/60 | Loss: 73.3811\n",
            "Epoch 50/60 | Loss: 73.4011\n",
            "Epoch 51/60 | Loss: 73.3870\n",
            "Epoch 52/60 | Loss: 73.4008\n",
            "Epoch 53/60 | Loss: 73.3930\n",
            "Epoch 54/60 | Loss: 73.4026\n",
            "Epoch 55/60 | Loss: 73.3724\n",
            "Epoch 56/60 | Loss: 73.3693\n",
            "Epoch 57/60 | Loss: 73.3502\n",
            "Epoch 58/60 | Loss: 73.3764\n",
            "Epoch 59/60 | Loss: 73.3689\n",
            "Epoch 60/60 | Loss: 73.3423\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "model.eval()\n",
        "\n",
        "x, _ = next(iter(train_loader))\n",
        "x = x.to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_hat, _, _ = model.forward_training(x)\n",
        "\n",
        "#save recons (top row = original, bottom row = recon)\n",
        "comparison = torch.cat([x[:8], x_hat[:8]])\n",
        "vutils.save_image(comparison, \"reconstructions.png\", nrow=8)\n",
        "\n",
        "#sampling from z\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(32, latent_dim).to(DEVICE)\n",
        "\n",
        "    z = model.fcinv(z)\n",
        "    z = model.fcinv2(z)\n",
        "    z = z.view(z.size(0), 128, 4, 4)\n",
        "\n",
        "    samples = model.decoder(z)\n",
        "\n",
        "vutils.save_image(samples, \"samples.png\", nrow=8)\n",
        "\n",
        "print(\"Saved reconstructions.png and samples.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXtCWle9uwHz",
        "outputId": "a0100742-9de3-4d82-fa9c-5c25c9b816ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved reconstructions.png and samples.png\n"
          ]
        }
      ]
    }
  ]
}